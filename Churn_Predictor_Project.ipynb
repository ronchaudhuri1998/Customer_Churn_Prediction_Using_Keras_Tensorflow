{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading and Preprocessing:\n",
        "\n",
        "The program loads a customer churn dataset from Google Drive.\n",
        "Preprocessing includes separating the features (input) and target (output) variables and identifying categorical and numerical columns.\n",
        "Categorical variables are one-hot encoded, and numerical features are standardized using StandardScaler.\n",
        "The dataset is split into training, validation, and test sets for model evaluation.\n",
        "Model Definition:\n",
        "\n",
        "A function build_model(hp) is defined to create a neural network using two hidden layers.\n",
        "Hyperparameters such as the number of units in each layer, activation function, and learning rate are tuned dynamically through Keras Tuner.\n",
        "The model is compiled using the Adam optimizer with a binary cross-entropy loss function and accuracy as a performance metric.\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "The program leverages different hyperparameter optimization techniques, such as Random Search, Bayesian Optimization, and Hyperband, to explore the hyperparameter space and find the best model.\n",
        "The model is evaluated on a test set after tuning to assess its performance.\n",
        "Performance Evaluation:\n",
        "\n",
        "The evaluate_model function is used to assess the model on the test set by calculating the test loss.\n",
        "Results for different optimization techniques, such as test loss and time taken, are collected and displayed in a table for comparison."
      ],
      "metadata": {
        "id": "hoRBStIII-y9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M01a-jNoi2Zu",
        "outputId": "d81f2d9f-902d-448f-c612-25f31417beb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install keras-tuner\n",
        "!pip install keras-tuner -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import RandomSearch, BayesianOptimization, Hyperband\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'Churn_Modelling.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing: Separate features and target variable\n",
        "X = data.drop(columns=['RowNumber', 'CustomerId', 'Surname'])\n",
        "y = data['Exited']\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = ['Geography', 'Gender']\n",
        "numerical_features = X.drop(columns=categorical_features).columns.tolist()\n",
        "\n",
        "# Use ColumnTransformer to apply different preprocessing steps to categorical and numerical columns\n",
        "data_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Split into training (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Split temp data into 50% validation, 50% test (which results in 20% validation, 20% test of full data)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Apply preprocessing (only fit on training data)\n",
        "X_train_scaled = data_transformer.fit_transform(X_train)  # Fit only on training data\n",
        "X_val_scaled = data_transformer.transform(X_val)          # Transform validation data\n",
        "X_test_scaled = data_transformer.transform(X_test)        # Transform test data\n",
        "\n",
        "\n",
        "\n",
        "# Define the model-building function for tuning with two hidden layers and added 'sigmoid' as an activation choice\n",
        "#The function must be defined to dynamically create the neural network model with different hyperparameters.\n",
        "#The function takes hp (hyperparameter object) as input and builds the model based on the values provided during each trial.\n",
        "#The function returns a compiled model, which Keras Tuner uses for training and evaluation.\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First hidden layer\n",
        "    model.add(Dense(units=hp.Int('units_0', min_value=32, max_value=128, step=32),\n",
        "                    activation=hp.Choice('activation', values=['relu', 'tanh', 'sigmoid']),\n",
        "                    input_shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    # Second hidden layer\n",
        "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32),\n",
        "                    activation=hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to evaluate model on the test set and return loss\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    test_loss = model.evaluate(X_test, y_test, verbose=0)[0]  # Return only test loss\n",
        "    return test_loss\n",
        "\n",
        "# Set number of trials (consistent across all methods)\n",
        "max_trials = 10\n",
        "max_epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search with overwrite=True to force re-run\n",
        "random_tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=max_trials,\n",
        "    directory='random_search',\n",
        "    project_name='customer_churn',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "#The search() method of Keras Tuner starts the hyperparameter tuning process, where it tries different combinations of hyperparameters\n",
        "#and trains the model with those values.\n",
        "random_tuner.search(X_train_scaled, y_train, epochs=max_epochs, validation_data=(X_val_scaled, y_val), verbose=0)\n",
        "random_search_time = time.time() - start_time\n",
        "\n",
        "# Get best model and evaluate on the test set\n",
        "random_search_best_model = random_tuner.get_best_models(num_models=1)[0]\n",
        "random_search_test_loss = evaluate_model(random_search_best_model, X_test_scaled, y_test)\n",
        "random_search_best_hyperparameters = random_tuner.get_best_hyperparameters(num_trials=1)[0].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8jmcc_Ca0sI",
        "outputId": "56274167-b638-4312-8728-8a569a27019c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Optimization with overwrite=True\n",
        "bayesian_tuner = BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=max_trials,\n",
        "    directory='bayesian_opt',\n",
        "    project_name='customer_churn',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "bayesian_tuner.search(X_train_scaled, y_train, epochs=max_epochs, validation_data=(X_val_scaled, y_val), verbose=0)\n",
        "bayesian_search_time = time.time() - start_time\n",
        "\n",
        "# Get best model and evaluate on the test set\n",
        "bayesian_search_best_model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
        "bayesian_search_test_loss = evaluate_model(bayesian_search_best_model, X_test_scaled, y_test)\n",
        "bayesian_search_best_hyperparameters = bayesian_tuner.get_best_hyperparameters(num_trials=1)[0].values"
      ],
      "metadata": {
        "id": "eE-QlhsOa49N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperband with overwrite=True\n",
        "hyperband_tuner = Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=max_epochs,\n",
        "    directory='hyperband',\n",
        "    project_name='customer_churn',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "hyperband_tuner.search(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), verbose=0)\n",
        "hyperband_search_time = time.time() - start_time\n",
        "\n",
        "# Get best model and evaluate on the test set\n",
        "hyperband_search_best_model = hyperband_tuner.get_best_models(num_models=1)[0]\n",
        "hyperband_search_test_loss = evaluate_model(hyperband_search_best_model, X_test_scaled, y_test)\n",
        "hyperband_search_best_hyperparameters = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0].values"
      ],
      "metadata": {
        "id": "vzLXQ-THag7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust Pandas setting to display full DataFrame content\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Collecting the results\n",
        "results = {\n",
        "    \"Method\": [\"Random Search\", \"Bayesian Optimization\", \"Hyperband\"],\n",
        "    \"Test Loss\": [random_search_test_loss, bayesian_search_test_loss, hyperband_search_test_loss],\n",
        "    \"Time (seconds)\": [random_search_time, bayesian_search_time, hyperband_search_time],\n",
        "    \"Best Hyperparameters\": [\n",
        "        random_search_best_hyperparameters,\n",
        "        bayesian_search_best_hyperparameters,\n",
        "        hyperband_search_best_hyperparameters\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Creating a DataFrame to display the results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0azVtYza_Pa",
        "outputId": "a2c9ce11-8db3-47c3-d4a1-33b2d60d8806"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Method     Test Loss  Time (seconds)  \\\n",
            "0          Random Search  2.306187e-08       40.533493   \n",
            "1  Bayesian Optimization  6.437411e-07       38.662306   \n",
            "2              Hyperband  9.167569e-10       19.769247   \n",
            "\n",
            "                                                                                                                                                                       Best Hyperparameters  \n",
            "0                                                                                                             {'units_0': 128, 'activation': 'tanh', 'units_1': 128, 'learning_rate': 0.01}  \n",
            "1                                                                                                              {'units_0': 128, 'activation': 'tanh', 'units_1': 96, 'learning_rate': 0.01}  \n",
            "2  {'units_0': 96, 'activation': 'relu', 'units_1': 32, 'learning_rate': 0.01, 'tuner/epochs': 3, 'tuner/initial_epoch': 1, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0001'}  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bayesian_tuner.results_summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqKaOaDcmBZM",
        "outputId": "e2f9ad2d-d873-4120-a509-b2bc2b017f78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in bayesian_opt/customer_churn\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units_0: 128\n",
            "activation: tanh\n",
            "units_1: 96\n",
            "learning_rate: 0.01\n",
            "Score: 4.213825661736337e-07\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: tanh\n",
            "units_1: 64\n",
            "learning_rate: 0.01\n",
            "Score: 6.465636943175923e-06\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: relu\n",
            "units_1: 128\n",
            "learning_rate: 0.001\n",
            "Score: 0.00010782071331050247\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: sigmoid\n",
            "units_1: 96\n",
            "learning_rate: 0.01\n",
            "Score: 0.00015658655320294201\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: relu\n",
            "units_1: 96\n",
            "learning_rate: 0.001\n",
            "Score: 0.00015830383927095681\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: relu\n",
            "units_1: 64\n",
            "learning_rate: 0.001\n",
            "Score: 0.00029572879429906607\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: tanh\n",
            "units_1: 128\n",
            "learning_rate: 0.0001\n",
            "Score: 0.0211542546749115\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: relu\n",
            "units_1: 32\n",
            "learning_rate: 0.0001\n",
            "Score: 0.10269074887037277\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: tanh\n",
            "units_1: 32\n",
            "learning_rate: 0.0001\n",
            "Score: 0.10608026385307312\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: relu\n",
            "units_1: 32\n",
            "learning_rate: 0.0001\n",
            "Score: 0.1420861929655075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperband_tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKADFrCImWce",
        "outputId": "86fa9796-4a9a-4177-ad3b-bfc3cc130a49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in hyperband/customer_churn\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 0003 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: relu\n",
            "units_1: 32\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 3\n",
            "tuner/initial_epoch: 1\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0001\n",
            "Score: 1.1347520700866198e-09\n",
            "\n",
            "Trial 0001 summary\n",
            "Hyperparameters:\n",
            "units_0: 96\n",
            "activation: relu\n",
            "units_1: 32\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 1\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 8.721873200556729e-06\n",
            "\n",
            "Trial 0000 summary\n",
            "Hyperparameters:\n",
            "units_0: 64\n",
            "activation: relu\n",
            "units_1: 32\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 1\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 4.3547366658458486e-05\n",
            "\n",
            "Trial 0002 summary\n",
            "Hyperparameters:\n",
            "units_0: 32\n",
            "activation: relu\n",
            "units_1: 64\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 1\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 5.019347372581251e-05\n",
            "\n",
            "Trial 0005 summary\n",
            "Hyperparameters:\n",
            "units_0: 128\n",
            "activation: relu\n",
            "units_1: 96\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 3\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.00015148591774050146\n",
            "\n",
            "Trial 0004 summary\n",
            "Hyperparameters:\n",
            "units_0: 128\n",
            "activation: sigmoid\n",
            "units_1: 128\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 3\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.003761615138500929\n"
          ]
        }
      ]
    }
  ]
}